# Red Witch Design Control Plan

The Red Witch Project will comply with the Ontario [Digital and Data Directive](https://www.ontario.ca/page/ontarios-digital-and-data-directive-2021) by adhering to the [Digital Service Standard](https://www.ontario.ca/page/digital-service-standard) and the guidelines established in the [Service Design Playbook](https://www.ontario.ca/page/service-design-playbook).

## **Design Control Plan as per DSS**

### **1. Software Development Lifecycle (SDLC) Phases**

#### **Discovery Phase**
   - **User Research**: Conduct thorough user research to understand potential users and their needs.
   - **Primary User Groups**: Identify and document primary user groups, personas, needs, and expectations.
   - **Existing Services**: Check if there are existing or non-governmental services that meet user needs.
   - **Policies and Barriers**: Identify policies and other barriers that will make meeting user needs difficult.
   - **Findings Documentation**: Document all findings, including user research results, market/regulatory/feasibility research.
   - **Product Manager**: Designate a product manager.
   - **Agile Workflow**: Establish an [agile](https://www.ontario.ca/page/being-agile-ontario-public-service) workflow.

#### **Alpha Phase**
   - **Stakeholder Co-creation**: Work directly with end users and stakeholders to co-create solutions.
   - **Prototypes**: Build and test multiple prototypes of the service.
   - **User Testing**: Plan and conduct continuous user testing with real users.
   - **Technical and Financial Feasibility**: Demonstrate that the service is technically and financially feasible.
   - **Process Changes**: Identify existing processes or policies that need to change to support the service.
   - **Usability Report**: Create a usability report based on user testing.
   - **Journey Map**: Develop a journey map of the user experience.
   - **Initial Project Plan**: Create an initial project plan including technical approach, financial estimate, market surveillance, service, support, and updates.

#### **Beta Phase**
   - **Minimum Viable Product (MVP)**: Build a minimum viable product that can be used by the public in a live environment.
   - **Continuous Testing**: Continuously test the service with users to collect feedback and discover insights.
   - **Device Validation**: Perform device validation on all platforms.
   - **Accessibility Testing**: Include accessibility-challenged user testers using [inclusive design cards](https://www.ontario.ca/page/inclusive-design-cards).
   - **Key Performance Indicators (KPIs)**: Measure the service against key performance indicators.
   - **Technical and Process Challenges**: Resolve any remaining technical or process-related challenges.
   - **Privacy and Security Report**: Create and maintain a privacy and security report.
   - **Automated Testing**: Implement automated testing processes.
   - **WCAG Success Criteria**: Include [WCAG success criteria](https://www.w3.org/TR/WCAG21/) in UI testing.
   - **Consistent Branding**: Use consistent branding leveraging the [Ontario Design System](https://designsystem.ontario.ca/).
   - **Maintenance Plan**: Develop plans for maintenance releases, outages, service monitoring.

#### **Live Phase**
   - **Service and Maintenance**: Continue service and maintenance post-launch.
   - **Ongoing User Research**: Conduct ongoing user research and usability testing every three to four months.
   - **Improvements and Updates**: Continue building features from the backlog and releasing improvements to the service.
   - **Service Success Communication**: Communicate and celebrate the successes of the service.
   - **Compliance with DSS**: Ensure the service continues to meet the Digital Service Standard.
   - **Web Analytics**: Employ web analytics for product surveillance.
   - **User Feedback**: Evaluate user complaints from all sources and respond appropriately.
   - **Performance Metrics**: Calculate and monitor performance metrics.
   - **Recovery Plan**: Create a recovery plan for disastrous data loss.
   - **Open Data**: Publish useful and open data in accordance with the [Open Data Guidebook](https://www.ontario.ca/document/open-data-guidebook-guide-open-data-directive-2015).

### **2. Methodologies and Tools**
- **Agile Methodology**: Utilize [agile methodology](https://www.ontario.ca/page/being-agile-ontario-public-service) for iterative development and frequent updates.
- **Consistent Branding**: Leverage the [Ontario Design System](https://designsystem.ontario.ca/) for consistent UI and UX design.
- **Accessibility**: Ensure accessibility by including [WCAG success criteria](https://www.w3.org/TR/WCAG21/) in UI testing and using inclusive design practices.

### **3. Post-Launch Considerations**
- **Support and Maintenance**: Ensure continuous support and maintenance, including frequent updates and rollbacks.
- **Usability Testing**: Conduct regular usability testing as part of the support plan post-launch.
- **Service Performance**: Monitor service performance using web analytics and respond to user feedback.

### **4. Risk Management**
- **Quality and Security Plans**: Create plans for frequent iterations, including rollbacks and security fixes.
- **Server and Device Updates**: Establish processes for updating server software and client devices.
- **Recovery Plan**: Develop a recovery plan for catastrophic data loss.
- **Privacy and Security**: Maintain a comprehensive privacy and security report.

### **5. Data Management**
- **Open Data**: Ensure data is accurate, timely, and released in an open format according to the [Open Data Guidebook](https://www.ontario.ca/document/open-data-guidebook-guide-open-data-directive-2015).
- **Data Security**: Ensure data remains secure and managed responsibly.
- **Performance Metrics**: Define and calculate performance metrics early in the design process and monitor continuously.

### **6. Digital Governance**
- **Compliance**: Ensure all service development and delivery activities comply with applicable regulations and standards.
- **Assessment Against DSS**: Evaluate against the Digital Service Standard at each phase in the design cycle.
- **Governance Processes**: Participate in governance processes like Digital First Assessments and Architecture Review Board as required.

### **7. Measurement and Evaluation**
- **Evaluation and Analytics**: Establish evaluation and analytics approaches to support continuous improvement.
- **Service Analytics**: Use service analytics to understand user behavior and improve service.
- **Realization of Benefits**: Monitor and report on the realization and sustainment of benefits.

By addressing these elements comprehensively, Red Witch Design Control will align closely with the Ontario Service Design Framework, ensuring compliance and effective service delivery.






----


1. Design Control SOP

Purpose: Define how design changes are made, approved, and tracked.

Scope: Applies across projects (or across all products under your organization’s control).

Focus: Process steps — submit change, review, approve, implement, verify, document traceability.

Outcome: Ensures any design change is compliant and auditable.

So the Design Control SOP doesn’t tell you the content of the project (e.g., what features Red Witch has); it tells you how to execute a design change safely.
